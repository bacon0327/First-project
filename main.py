from bert_command_classifier import BertCommandClassifier
from ai_gomoku import GomokuAI
import collections
import json
import numpy as np
import pyaudio
import re
import sys
import time
import wave
import webrtcvad
import whisper

AUDIO_PATH = "record_temp.wav"
JSON_OUTPUT_PATH = "output_bert.json"
TYPE_OUTPUT_PATH = "type.json"

print("è¼‰å…¥ Whisper æ¨¡å‹...")
whisper_model = whisper.load_model("large", device="cuda")
print("âœ… Whisper è¼‰å…¥å®Œæˆã€‚")
print("è¼‰å…¥ BERT åˆ†é¡å™¨...")
bert_classifier = BertCommandClassifier("best_model")
print("âœ… BERT åˆ†é¡å™¨è¼‰å…¥å®Œæˆã€‚")

gomoku_ai = GomokuAI()
ai_enabled = True

zh_to_arabic = {
    'ä¸€': '1', 'äºŒ': '2', 'ä¸‰': '3', 'å››': '4', 'äº”': '5',
    'å…­': '6', 'ä¸ƒ': '7', 'å…«': '8', 'ä¹': '9', 'å': '10',
    'åä¸€': '11', 'åäºŒ': '12', 'åä¸‰': '13', 'åå››': '14', 'åäº”': '15'
}
sorted_zh = sorted(zh_to_arabic.keys(), key=lambda x: -len(x))
zh_number_pattern = '|'.join(sorted_zh)
arabic_pattern = r'\d{1,2}'
mixed_pattern = re.compile(rf'({zh_number_pattern}|{arabic_pattern}|å±±)ä¹‹({zh_number_pattern}|{arabic_pattern}|å±±)')

def normalize_coordinate(match):
    left = match.group(1).replace("å±±", "ä¸‰")
    right = match.group(2).replace("å±±", "ä¸‰")
    left_num = zh_to_arabic.get(left, left)
    right_num = zh_to_arabic.get(right, right)
    return f"{int(left_num)}ä¹‹{int(right_num)}"

def normalize_text(text):
    text = (
        text.replace("é»‘ç´™", "é»‘å­")
            .replace("ç™½ç´™", "ç™½å­")
            .replace("é»‘ç´«", "é»‘å­")
            .replace("ç™½ç´«", "ç™½å­")
            .replace("é»‘æ£‹", "é»‘å­")
            .replace("ç™½æ£‹", "ç™½å­")
    )

    # åŒéŸ³èª¤å­—ä¿®æ­£
    for wrong in ["å‚¢ä¿±", "åŠ åŠ‡", "å‚¢å…·", "å®¶ä¿±"]:
        text = text.replace(wrong, "å®¶å…·")

    # æ–¹ä½èª¤å­—ä¿®æ­£
    direction_aliases = {
        "æ¡Œä¸Šè§’": "å·¦ä¸Šè§’",
        "æ¡Œä¸‹è§’": "å·¦ä¸‹è§’",
        "å³ä¸Šæ–¹": "å³ä¸Šè§’",
        "å³ä¸‹æ–¹": "å³ä¸‹è§’",
        "å·¦ä¸Šæ–¹": "å·¦ä¸Šè§’",
        "å·¦ä¸‹æ–¹": "å·¦ä¸‹è§’",
        "å…¥ä¸‹è§’": "å³ä¸‹è§’",
        "ç”±ä¸‹è§’": "å³ä¸‹è§’",
        "ä¸­ç›£": "ä¸­é–“",
        "ä¸­éµ": "ä¸­é–“"
    }
    for wrong, correct in direction_aliases.items():
        text = text.replace(wrong, correct)

    text = mixed_pattern.sub(normalize_coordinate, text)
    return text

def record_and_segment(out_path=AUDIO_PATH):
    FORMAT = pyaudio.paInt16
    CHANNELS = 1
    RATE = 16000
    CHUNK_DURATION_MS = 30
    PADDING_DURATION_MS = 1500
    CHUNK_SIZE = int(RATE * CHUNK_DURATION_MS / 1000)
    NUM_PADDING_CHUNKS = int(PADDING_DURATION_MS / CHUNK_DURATION_MS)
    vad = webrtcvad.Vad(2)

    audio = pyaudio.PyAudio()
    stream = audio.open(format=FORMAT, channels=CHANNELS,
                        rate=RATE, input=True, frames_per_buffer=CHUNK_SIZE)
    print(" é–‹å§‹éŒ„éŸ³ï¼Œè«‹èªªè©±...ï¼ˆCtrl+CçµæŸï¼‰")

    try:
        ring_buffer = collections.deque(maxlen=NUM_PADDING_CHUNKS)
        triggered = False
        voiced_frames = []
        dots = 0
        frame_count = 0
        start_time = None
        
        while True:
            frame = stream.read(CHUNK_SIZE, exception_on_overflow=False)
            is_speech = vad.is_speech(frame, RATE)
            
            if triggered:
                frame_count += 1
                if frame_count >= 3:
                    sys.stdout.write('\réŒ„éŸ³ä¸­' + '.' * (dots % 4) + '   ')
                    sys.stdout.flush()
                    dots += 1
                    frame_count = 0
            
            if not triggered:
                ring_buffer.append((frame, is_speech))
                num_voiced = len([f for f, speech in ring_buffer if speech])
                if num_voiced > 0.5 * ring_buffer.maxlen:
                    triggered = True
                    start_time = time.time()
                    voiced_frames.extend([f for f, s in ring_buffer])
                    ring_buffer.clear()
            else:
                voiced_frames.append(frame)
                ring_buffer.append((frame, is_speech))
                num_unvoiced = len([f for f, speech in ring_buffer if not speech])
                if num_unvoiced > 0.8 * ring_buffer.maxlen and (time.time() - start_time) > 1.0:
                    print("\nåµæ¸¬åˆ°éœéŸ³ï¼Œè‡ªå‹•å­˜æª”")
                    wf = wave.open(out_path, 'wb')
                    wf.setnchannels(CHANNELS)
                    wf.setsampwidth(audio.get_sample_size(FORMAT))
                    wf.setframerate(RATE)
                    wf.writeframes(b''.join(voiced_frames))
                    wf.close()
                    break
    except KeyboardInterrupt:
        print("\néŒ„éŸ³æ‰‹å‹•çµæŸã€‚")
    finally:
        stream.stop_stream()
        stream.close()
        audio.terminate()
        print("éŒ„éŸ³å·²çµæŸã€‚")

def ask_type():
    print("è«‹èªªæ˜ä½ è¦åŸ·è¡Œçš„æ“ä½œï¼ˆä¾‹å¦‚ï¼šæˆ‘è¦æ§åˆ¶å®¶å…·ã€æˆ‘è¦ä¸‹äº”å­æ£‹ï¼‰...")
    record_and_segment(AUDIO_PATH)
    result = whisper_model.transcribe(AUDIO_PATH, language="zh")
    text = normalize_text(result["text"].strip())
    print(f"è¾¨è­˜çµæœï¼š{text}")
    if "å®¶å…·" in text:
        print("é€²å…¥å®¶å…·æ§åˆ¶æ¨¡å¼")
        mode = "furniture"
    else:
        print("é€²å…¥äº”å­æ£‹æ¨¡å¼ï¼ˆé è¨­ï¼‰")
        mode = "gomoku"

    with open(TYPE_OUTPUT_PATH, "w", encoding="utf-8") as f:
        json.dump({"type": mode}, f, ensure_ascii=False, indent=2)
    return mode

def ask_gomoku_type():
    print("è«‹å•æ˜¯é›™äººå°æˆ°é‚„æ˜¯äººæ©Ÿå°æˆ°ï¼Ÿ")
    record_and_segment(AUDIO_PATH)
    result = whisper_model.transcribe(AUDIO_PATH, language="zh")
    text = normalize_text(result["text"].strip())
    print(f"è¾¨è­˜çµæœï¼š{text}")
    if "é›™äºº" in text or "å…©äºº" in text:
        print("æ¨¡å¼ï¼šé›™äººå°æˆ°")
        return False
    print("æœªåµæ¸¬åˆ°æœ‰æ•ˆè¼¸å‡ºï¼Œé è¨­ç‚ºäººæ©Ÿå°æˆ°")
    return True

def run_once_and_return_json():
    record_and_segment(AUDIO_PATH)
    result = whisper_model.transcribe(
        AUDIO_PATH,
        language="zh",
        initial_prompt= (
            "é€™æ˜¯ä¸€å€‹èªéŸ³æŒ‡ä»¤ç³»çµ±ï¼ŒåŒ…å«å®¶å…·æ§åˆ¶èˆ‡äº”å­æ£‹ã€‚"
            "ç©å®¶æœƒèªªå‡ºåƒæ˜¯ã€Œé»‘å­ä¸‹åœ¨ä¸‰ä¹‹ä¸‰ã€ã€ã€Œç™½å­æ”¾åœ¨äº”ä¹‹ä¸ƒã€é€™é¡èªå¥ã€‚"
            "è«‹ç¢ºä¿é—œéµè©å¦‚ï¼šé»‘å­ã€ç™½å­ã€ä¸‹ã€æ”¾ã€ä¹‹ã€æ£‹ç›¤åº§æ¨™ï¼ˆä¸‰ä¹‹ä¸‰ã€äº”ä¹‹ä¸ƒç­‰ï¼‰éƒ½èƒ½æ­£ç¢ºè¾¨è­˜ã€‚"
            "é—œéµè©é‚„æœ‰ï¼šæ‚”æ£‹ã€å›ä¸Šä¸€æ­¥ã€çµæŸéŠæˆ²ã€é‡é–‹éŠæˆ²"
        )
    )
    text = normalize_text(result["text"].strip())
    print(f"è¾¨è­˜çµæœï¼š{text}")
    label = bert_classifier.predict_label(text)
    print(f"æŒ‡ä»¤é¡å‹:label = {label}")

    if label == 0:
        print("éæŒ‡ä»¤èªå¥ï¼š", text)
        return None
    elif label == 1:
        return bert_classifier.to_gomoku_json(text)
    elif label == 2:
        return bert_classifier.to_game_control_json(text)
    elif label == 3:
        return bert_classifier.to_furniture_control_json(text)

    return None

def main():
    print("=== Whisper + BERT æŒ‡ä»¤è¾¨è­˜ç³»çµ±å•Ÿå‹• ===")
    mode = ask_type()

    if mode == "furniture":
        print("å®¶å…·æ§åˆ¶æ¨¡å¼å·²å•Ÿå‹•ã€‚")
    else:
        ai_enabled = ask_gomoku_type()
        print(f"æ¨¡å¼é¸æ“‡å®Œæˆï¼š{'èˆ‡ AI å°æˆ°' if ai_enabled else 'é›™äººå°æˆ°'}")

    try:
        while True:
            print("\nğŸ¤ è«‹èªªå‡ºèªéŸ³æŒ‡ä»¤...")
            result = run_once_and_return_json()
            if result is not None:
                with open(JSON_OUTPUT_PATH, "w", encoding="utf-8") as f:
                    json.dump(result, f, ensure_ascii=False, indent=2)
                print("æŒ‡ä»¤å·²å¯«å…¥ JSONï¼š", result)

                if mode == "gomoku" and ai_enabled:
                    ai_move = gomoku_ai.get_best_move()
                    gomoku_ai.apply_json_move(ai_move)
                    with open(JSON_OUTPUT_PATH, "w", encoding="utf-8") as f:
                        json.dump(ai_move, f, ensure_ascii=False, indent=2)
                    print("AI å·²ä¸‹æ£‹ä¸¦æ›´æ–° JSONï¼š", ai_move)
    except KeyboardInterrupt:
        print("ä½¿ç”¨è€…ä¸­æ­¢ï¼Œç¨‹å¼çµæŸ")

if __name__ == "__main__":
    main()