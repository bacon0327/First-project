from bert_command_classifier import BertCommandClassifier
from ai_gomoku import GomokuAI
import collections
import json
import numpy as np
import pyaudio
import re
import sys
import time
import wave
import webrtcvad
import whisper

AUDIO_PATH = "record_temp.wav"
lab2_OUTPUT_PATH = "D:/Program Files/code/GomokuGame/Assets/StreamingAssets/step.json"
lab3_OUTPUT_PATH = "D:/Program Files/code/GomokuGame/Assets/StreamingAssets/furniture_command.json"
TYPE_OUTPUT_PATH = "D:/Program Files/code/GomokuGame/Assets/StreamingAssets/change.json"
last_label = None  # æ”¾åœ¨ main.py æœ€ä¸Šæ–¹åšç‚ºå…¨åŸŸè®Šæ•¸

print("è¼‰å…¥ Whisper æ¨¡å‹...")
whisper_model = whisper.load_model("large-v3", device="cuda")
print("âœ… Whisper è¼‰å…¥å®Œæˆã€‚")
print("è¼‰å…¥ BERT åˆ†é¡å™¨...")
bert_classifier = BertCommandClassifier("best_model")
print("âœ… BERT åˆ†é¡å™¨è¼‰å…¥å®Œæˆã€‚")

gomoku_ai = GomokuAI()
ai_enabled = True

zh_to_arabic = {
    'ä¸€': '1', 'äºŒ': '2', 'ä¸‰': '3', 'å››': '4', 'äº”': '5',
    'å…­': '6', 'ä¸ƒ': '7', 'å…«': '8', 'ä¹': '9', 'å': '10',
    'åä¸€': '11', 'åäºŒ': '12', 'åä¸‰': '13', 'åå››': '14', 'åäº”': '15'
}
sorted_zh = sorted(zh_to_arabic.keys(), key=lambda x: -len(x))
zh_number_pattern = '|'.join(sorted_zh)
arabic_pattern = r'\d{1,2}'
mixed_pattern = re.compile(rf'({zh_number_pattern}|{arabic_pattern}|å±±)ä¹‹({zh_number_pattern}|{arabic_pattern}|å±±)')

def normalize_coordinate(match):
    left = match.group(1).replace("å±±", "ä¸‰")
    right = match.group(2).replace("å±±", "ä¸‰")
    left_num = zh_to_arabic.get(left, left)
    right_num = zh_to_arabic.get(right, right)
    return f"{int(left_num)}ä¹‹{int(right_num)}"

def normalize_text(text):
    text = (
        text.replace("é»‘ç´™", "é»‘å­")
            .replace("ç™½ç´™", "ç™½å­")
            .replace("é»‘ç´«", "é»‘å­")
            .replace("ç™½ç´«", "ç™½å­")
            .replace("é»‘æ£‹", "é»‘å­")
            .replace("ç™½æ£‹", "ç™½å­")
    )

    # åŒéŸ³èª¤å­—ä¿®æ­£
    for wrong in ["å‚¢ä¿±", "åŠ åŠ‡", "å‚¢å…·", "å®¶ä¿±"]:
        text = text.replace(wrong, "å®¶å…·")

    # æ–¹ä½èª¤å­—ä¿®æ­£
    direction_aliases = {
        "æ¡Œä¸Šè§’": "å·¦ä¸Šè§’",
        "æ¡Œä¸‹è§’": "å·¦ä¸‹è§’",
        "å³ä¸Šæ–¹": "å³ä¸Šè§’",
        "å³ä¸‹æ–¹": "å³ä¸‹è§’",
        "å·¦ä¸Šæ–¹": "å·¦ä¸Šè§’",
        "å·¦ä¸‹æ–¹": "å·¦ä¸‹è§’",
        "å…¥ä¸‹è§’": "å³ä¸‹è§’",
        "ç”±ä¸‹è§’": "å³ä¸‹è§’",
        "ä¸­ç›£": "ä¸­é–“",
        "ä¸­éµ": "ä¸­é–“"
    }
    for wrong, correct in direction_aliases.items():
        text = text.replace(wrong, correct)

    text = mixed_pattern.sub(normalize_coordinate, text)
    return text

def record_and_segment(out_path=AUDIO_PATH):
    FORMAT = pyaudio.paInt16
    CHANNELS = 1
    RATE = 16000
    CHUNK_DURATION_MS = 30
    PADDING_DURATION_MS = 1500
    CHUNK_SIZE = int(RATE * CHUNK_DURATION_MS / 1000)
    NUM_PADDING_CHUNKS = int(PADDING_DURATION_MS / CHUNK_DURATION_MS)
    vad = webrtcvad.Vad(2)

    audio = pyaudio.PyAudio()
    stream = audio.open(format=FORMAT, channels=CHANNELS,
                        rate=RATE, input=True, frames_per_buffer=CHUNK_SIZE)
    print(" é–‹å§‹éŒ„éŸ³ï¼Œè«‹èªªè©±...ï¼ˆCtrl+CçµæŸï¼‰")

    try:
        ring_buffer = collections.deque(maxlen=NUM_PADDING_CHUNKS)
        triggered = False
        voiced_frames = []
        dots = 0
        frame_count = 0
        start_time = None
        
        while True:
            frame = stream.read(CHUNK_SIZE, exception_on_overflow=False)
            is_speech = vad.is_speech(frame, RATE)
            
            if triggered:
                frame_count += 1
                if frame_count >= 3:
                    sys.stdout.write('\réŒ„éŸ³ä¸­' + '.' * (dots % 4) + '   ')
                    sys.stdout.flush()
                    dots += 1
                    frame_count = 0
            
            if not triggered:
                ring_buffer.append((frame, is_speech))
                num_voiced = len([f for f, speech in ring_buffer if speech])
                if num_voiced > 0.5 * ring_buffer.maxlen:
                    triggered = True
                    start_time = time.time()
                    voiced_frames.extend([f for f, s in ring_buffer])
                    ring_buffer.clear()
            else:
                voiced_frames.append(frame)
                ring_buffer.append((frame, is_speech))
                num_unvoiced = len([f for f, speech in ring_buffer if not speech])
                if num_unvoiced > 0.8 * ring_buffer.maxlen and (time.time() - start_time) > 1.0:
                    print("\nåµæ¸¬åˆ°éœéŸ³ï¼Œè‡ªå‹•å­˜æª”")
                    wf = wave.open(out_path, 'wb')
                    wf.setnchannels(CHANNELS)
                    wf.setsampwidth(audio.get_sample_size(FORMAT))
                    wf.setframerate(RATE)
                    wf.writeframes(b''.join(voiced_frames))
                    wf.close()
                    break
    except KeyboardInterrupt:
        print("\néŒ„éŸ³æ‰‹å‹•çµæŸã€‚")
    finally:
        stream.stop_stream()
        stream.close()
        audio.terminate()
        print("éŒ„éŸ³å·²çµæŸã€‚")

def ask_type():
    print("è«‹èªªæ˜ä½ è¦åŸ·è¡Œçš„æ“ä½œï¼ˆä¾‹å¦‚ï¼šæˆ‘è¦æ§åˆ¶å®¶å…·ã€æˆ‘è¦ä¸‹äº”å­æ£‹ï¼‰...")

    with open(TYPE_OUTPUT_PATH, "w", encoding="utf-8") as f:
        json.dump({"type":""}, f, ensure_ascii=False, indent=2)

    record_and_segment(AUDIO_PATH)
    result = whisper_model.transcribe(AUDIO_PATH, language="zh")
    text = normalize_text(result["text"].strip())
    print(f"è¾¨è­˜çµæœï¼š{text}")
    if "å®¶å…·" in text:
        print("é€²å…¥å®¶å…·æ§åˆ¶æ¨¡å¼")
        mode = "furniture"
    else:
        print("é€²å…¥äº”å­æ£‹æ¨¡å¼ï¼ˆé è¨­ï¼‰")
        mode = "gomoku"

    with open(TYPE_OUTPUT_PATH, "w", encoding="utf-8") as f:
        json.dump({"type": mode}, f, ensure_ascii=False, indent=2)
    return mode

def ask_gomoku_type():
    print("è«‹å•æ˜¯é›™äººå°æˆ°é‚„æ˜¯äººæ©Ÿå°æˆ°ï¼Ÿ")
    record_and_segment(AUDIO_PATH)
    result = whisper_model.transcribe(AUDIO_PATH, language="zh")
    text = normalize_text(result["text"].strip())
    print(f"è¾¨è­˜çµæœï¼š{text}")
    if "é›™äºº" in text or "å…©äºº" in text:
        print("æ¨¡å¼ï¼šé›™äººå°æˆ°")
        return False
    print("æœªåµæ¸¬åˆ°æœ‰æ•ˆè¼¸å‡ºï¼Œé è¨­ç‚ºäººæ©Ÿå°æˆ°")
    return True

def confirm_switch_mode():
    print("è«‹å•æ‚¨è¦åˆ‡æ›æ¨¡å¼å—ï¼Ÿï¼ˆæ˜¯/å¦ï¼‰")
    record_and_segment(AUDIO_PATH)
    result = whisper_model.transcribe(AUDIO_PATH, language="zh")
    text = result["text"].strip()
    print(f"è¾¨è­˜çµæœï¼š{text}")
    if "æ˜¯" in text or "æ˜¯çš„" in text or "å¥½" in text:
        return True
    return False

def run_once_and_return_json():

    global last_label
    global last_recognized_text # ä½¿ç”¨å…¨åŸŸè®Šæ•¸

    record_and_segment(AUDIO_PATH)
    result = whisper_model.transcribe(
        AUDIO_PATH,
        language="zh",
        initial_prompt= (
            "é€™æ˜¯ä¸€å€‹èªéŸ³æŒ‡ä»¤ç³»çµ±ï¼ŒåŒ…å«å®¶å…·æ§åˆ¶èˆ‡äº”å­æ£‹ã€‚"
            "ç©å®¶æœƒèªªå‡ºåƒæ˜¯ã€Œé»‘å­ä¸‹åœ¨ä¸‰ä¹‹ä¸‰ã€ã€ã€Œç™½å­æ”¾åœ¨äº”ä¹‹ä¸ƒã€é€™é¡èªå¥ã€‚"
            "è«‹ç¢ºä¿é—œéµè©å¦‚ï¼šé»‘å­ã€ç™½å­ã€ä¸‹ã€æ”¾ã€ä¹‹ã€æ£‹ç›¤åº§æ¨™ï¼ˆä¸‰ä¹‹ä¸‰ã€äº”ä¹‹ä¸ƒç­‰ï¼‰éƒ½èƒ½æ­£ç¢ºè¾¨è­˜ã€‚"
            "é—œéµè©é‚„æœ‰ï¼šæ‚”æ£‹ã€å›ä¸Šä¸€æ­¥ã€çµæŸéŠæˆ²ã€é‡é–‹éŠæˆ²"
        )
    )
    text = normalize_text(result["text"].strip())
    last_recognized_text = text # å°‡åŸå§‹æ–‡æœ¬å„²å­˜åˆ°å…¨åŸŸè®Šæ•¸
    print(f"è¾¨è­˜çµæœï¼š{text}")
    label = bert_classifier.predict_label(text)
    last_label = label
    print(f"æŒ‡ä»¤é¡å‹:label = {label}")

    if label == 0:
        print("éæŒ‡ä»¤èªå¥ï¼š", text)
        return None
    elif label == 1:
        return bert_classifier.to_gomoku_json(text)
    elif label == 2:
        return bert_classifier.to_game_control_json(text)
    elif label == 3:
        return bert_classifier.to_furniture_control_json(text)

    return None

def main():

    global ai_enabled
    global last_label # ç¢ºä¿åœ¨ main ä¸­å¯ä»¥è®€å–åˆ° last_label
    global last_recognized_text # ç¢ºä¿åœ¨ main ä¸­å¯ä»¥è®€å–åˆ° last_recognized_text
    
    switch_keywords = ["åˆ‡æ›", "åˆ‡æ¢"]

    print("=== Whisper + BERT æŒ‡ä»¤è¾¨è­˜ç³»çµ±å•Ÿå‹• ===")

    mode = ask_type()

    if mode == "furniture":
        print("å®¶å…·æ§åˆ¶æ¨¡å¼å·²å•Ÿå‹•ã€‚")
    else:
        ai_enabled = ask_gomoku_type()
        print(f"æ¨¡å¼é¸æ“‡å®Œæˆï¼š{'èˆ‡ AI å°æˆ°' if ai_enabled else 'é›™äººå°æˆ°'}")

    try:
        while True:
            print("\nğŸ¤ è«‹èªªå‡ºèªéŸ³æŒ‡ä»¤...")
            result = run_once_and_return_json()

            if last_label == 0 and any(keyword in last_recognized_text for keyword in switch_keywords):
                if confirm_switch_mode():
                    if mode == "furniture":
                        mode = "gomoku"
                        print("æ¨¡å¼å·²åˆ‡æ›ç‚ºäº”å­æ£‹ã€‚")
                        ai_enabled = ask_gomoku_type()
                        print(f"æ¨¡å¼é¸æ“‡å®Œæˆï¼š{'èˆ‡ AI å°æˆ°' if ai_enabled else 'é›™äººå°æˆ°'}")
                    else:
                        mode = "furniture"
                        print("æ¨¡å¼å·²åˆ‡æ›ç‚ºå®¶å…·æ§åˆ¶ã€‚")
                    
                    # é€šçŸ¥ C# ç«¯åˆ‡æ›æ¨¡å¼
                    with open(TYPE_OUTPUT_PATH, "w", encoding="utf-8") as f:
                        json.dump({"type": mode}, f, ensure_ascii=False, indent=2)
                    continue # åˆ‡æ›æ¨¡å¼å¾Œï¼Œé‡æ–°é–‹å§‹å¾ªç’°ï¼Œé¿å…è™•ç†ç•¶å‰æŒ‡ä»¤
                else:
                    print("å·²å–æ¶ˆæ¨¡å¼åˆ‡æ›ã€‚")
                    continue # é‡æ–°é–‹å§‹å¾ªç’°ï¼Œç­‰å¾…ä¸‹ä¸€å€‹æŒ‡ä»¤
            
            if result is not None:
                # æª¢æŸ¥æ˜¯å¦è·¨æ¨¡å¼æŒ‡ä»¤ï¼ˆéœ€è¦åˆ‡æ›ï¼‰
                if mode == "gomoku" and last_label == 3:
                    print("âš ï¸ åµæ¸¬åˆ°å®¶å…·æŒ‡ä»¤ï¼Œç•¶å‰ç‚ºäº”å­æ£‹æ¨¡å¼")
                    if confirm_switch_mode():
                        mode = "furniture"
                        print("æ¨¡å¼å·²åˆ‡æ›ç‚ºå®¶å…·æ§åˆ¶")
                        with open(TYPE_OUTPUT_PATH, "w", encoding="utf-8") as f:
                            json.dump({"type": mode}, f, ensure_ascii=False, indent=2)
                    else:
                        print("ä½¿ç”¨è€…å–æ¶ˆåˆ‡æ›ï¼Œå¿½ç•¥æ­¤æ¬¡æŒ‡ä»¤")
                    continue  # è·³éæ­¤æ¬¡ result è™•ç†

                elif mode == "furniture" and last_label == 1:
                    print("âš ï¸ åµæ¸¬åˆ°äº”å­æ£‹æŒ‡ä»¤ï¼Œç•¶å‰ç‚ºå®¶å…·æ¨¡å¼")
                    if confirm_switch_mode():
                        mode = "gomoku"
                        print("æ¨¡å¼å·²åˆ‡æ›ç‚ºäº”å­æ£‹")
                        ai_enabled = ask_gomoku_type()
                        with open(TYPE_OUTPUT_PATH, "w", encoding="utf-8") as f:
                            json.dump({"type": mode}, f, ensure_ascii=False, indent=2)
                    else:
                        print("ä½¿ç”¨è€…å–æ¶ˆåˆ‡æ›ï¼Œå¿½ç•¥æ­¤æ¬¡æŒ‡ä»¤")
                    continue

                if mode == "gomoku":
                    with open(lab2_OUTPUT_PATH, "w", encoding="utf-8") as f:
                        json.dump(result, f, ensure_ascii=False, indent=2)
                    print("æŒ‡ä»¤å·²å¯«å…¥ JSONï¼š", result)

                    if ai_enabled and last_label == 1:
                        ai_move = gomoku_ai.get_best_move()
                        gomoku_ai.apply_json_move(ai_move)
                        time.sleep(1)
                        with open(lab2_OUTPUT_PATH, "w", encoding="utf-8") as f:
                            json.dump(ai_move, f, ensure_ascii=False, indent=2)
                        print("AI å·²ä¸‹æ£‹ä¸¦æ›´æ–° JSONï¼š", ai_move)

                elif mode == "furniture":
                    with open(lab3_OUTPUT_PATH, "w", encoding="utf-8") as f:
                        json.dump(result, f, ensure_ascii=False, indent=2)
                    print("æŒ‡ä»¤å·²å¯«å…¥ JSONï¼š", result)

    except KeyboardInterrupt:
        print("ä½¿ç”¨è€…ä¸­æ­¢ï¼Œç¨‹å¼çµæŸ")

if __name__ == "__main__":
    main()